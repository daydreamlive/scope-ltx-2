"""LTX2 pipeline configuration schema."""

from typing import ClassVar

from scope.core.pipelines.base_schema import BasePipelineConfig, ModeDefaults, height_field, width_field
from scope.core.pipelines.artifacts import HuggingfaceRepoArtifact


class LTX2Config(BasePipelineConfig):
    """Configuration for LTX2 text-to-video pipeline.

    LTX2 is a high-quality video generation model that generates videos from text prompts.
    This is a non-autoregressive model that generates complete videos in one shot.
    """

    # Pipeline metadata
    pipeline_id: ClassVar[str] = "ltx2"
    pipeline_name: ClassVar[str] = "LTX2"
    pipeline_description: ClassVar[str] = (
        "High-quality text-to-video generation with LTX2 transformer"
    )
    pipeline_version: ClassVar[str] = "0.1.0"
    docs_url: ClassVar[str | None] = "https://github.com/Lightricks/LTX-2"
    estimated_vram_gb: ClassVar[float | None] = 96.0  # ~48GB models + ~50GB activations
    requires_models: ClassVar[bool] = True
    artifacts = [
        HuggingfaceRepoArtifact(
            repo_id="Lightricks/LTX-2",
            files=[
                "ltx-2-19b-distilled.safetensors",
                "ltx-2-spatial-upscaler-x2-1.0.safetensors",
            ],
        ),
        HuggingfaceRepoArtifact(
            repo_id="google/gemma-3-12b-it",
            files=[
                "config.json",
                "generation_config.json",
                "model-00001-of-00005.safetensors",
                "model-00002-of-00005.safetensors",
                "model-00003-of-00005.safetensors",
                "model-00004-of-00005.safetensors",
                "model-00005-of-00005.safetensors",
                "model.safetensors.index.json",
                "processor_config.json",
                "preprocessor_config.json",
                "special_tokens_map.json",
                "tokenizer.json",
                "tokenizer.model",
                "tokenizer_config.json",
            ],
        )
    ]
    supports_lora: ClassVar[bool] = False
    supports_vace: ClassVar[bool] = False

    # UI capability metadata
    supports_cache_management: ClassVar[bool] = False
    supports_kv_cache_bias: ClassVar[bool] = False
    supports_quantization: ClassVar[bool] = True
    min_dimension: ClassVar[int] = 64
    modified: ClassVar[bool] = False
    recommended_quantization_vram_threshold: ClassVar[float | None] = 32.0
    
    # NOTE: The following flags are NOT in scope main but are used by the branch:
    # - supports_randomize_seed: ClassVar[bool] = True  # LTX2 is bidirectional
    # - supports_num_frames: ClassVar[bool] = True  # LTX2 supports configurable frames
    # These parameters still work via kwargs, just without UI controls in scope main.

    # Mode configuration - only supports text mode for now
    modes: ClassVar[dict[str, ModeDefaults]] = {"text": ModeDefaults(default=True)}

    # Prompt support
    supports_prompts: ClassVar[bool] = True

    # Resolution settings (LTX2 works best at these resolutions)
    # CRITICAL: Set to minimal values to fit in 96GB VRAM
    # Activations during denoising are NOT quantized and scale with resolution√óframes
    # Even with FP8 weights, activations use 70+ GB at high settings
    height: int = height_field(default=512)
    width: int = width_field(default=768)

    # Number of frames to generate
    # Reduced to 33 frames (~1.3 seconds) to fit in 96GB VRAM
    # Memory formula: ~1.5GB per frame at 512x768 resolution
    num_frames: int = 33

    # Frame rate for video generation
    frame_rate: float = 24.0

    # Memory optimization: Use FP8 quantization for transformer
    # According to official LTX-2 docs, this significantly reduces VRAM usage
    # Requires PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    use_fp8: bool = True

    # Randomize seed on every generation
    # LTX2 is bidirectional (not autoregressive), so each chunk is independent.
    # With a fixed seed, the same chunk is regenerated unless the prompt changes.
    # Enable this to get varied outputs between chunks.
    randomize_seed: bool = False
